# -*- coding: utf-8 -*-
"""handson 2.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1AauW680nWqE_MFe2ylmb0eigpQmn3C0E

# Installation and Setup
"""

!pip install gensim sentence-transformers

import re
import pandas as pd
import numpy as np
from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

from gensim.models import Word2Vec
from sentence_transformers import SentenceTransformer

import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.tokenize import word_tokenize
nltk.download('stopwords')
nltk.download('punkt_tab')

"""# Download and Import Dataset"""

df = pd.read_csv('/content/HandsOnTask1.csv', sep=',')

df = pd.read_csv('/content/world_tourism_economy_data.csv', sep=',')

df_tourist = pd.read_csv('HandsOnTask1.csv')
df_tourist.head()

df_travel = pd.read_csv('world_tourism_economy_data.csv', sep=',')
display(df_travel.head())

"""# Text Processing"""

df_tourist['text'] = df_course['name'] +  ' ' + df_tourist['capital'] + ' ' + df_tourist['population'].astype(str) + ' ' + df_tourist['area'].astype(str)
df_tourist = df_tourist[['name', 'text']]
display(df_tourist.head())

df_travel['text2'] = df_travel['country'] + ' ' + df_travel['country_code'] + ' ' + df_travel['year'].astype(str)
df_travel = df_travel[['country', 'text2']]
display(df_travel.head())

def clean_noise(text):
  text = re.sub(r'<.*?>', ' ', text) # Hapus tag HTML
  text = re.sub(r'https?://\S+|www\.\S+', ' ', text) # Hapus URL
  text = re.sub(r'#\w+', ' ', text) # Hapus hashtag
  text = re.sub(r'[^\w\s]', ' ', text) # Hapus tanda baca dan karakter khusus
  text = re.sub(r'\d+', ' ', text) # Hapus angka
  text = re.sub(r'\s+', ' ', text).strip() # Hapus spasi berlebih
  return text

def remove_stopwords(text):
  stopwords_set = set(stopwords.words('english'))
  words = text.split()
  filtered_words = [word for word in words if word.lower() not in stopwords_set]
  return ' '.join(filtered_words)

def stem_text(text):
  stemmer = PorterStemmer()
  words = text.split()
  stemmed_words = [stemmer.stem(word) for word in words]
  return ' '.join(stemmed_words)

def process(text):
    text = str(text) # Convert to string to handle non-string types
    text = text.lower()
    text = clean_noise(text)
    text = remove_stopwords(text)
    text = stem_text(text)
    return text

df_tourist['text'] = df_tourist['text'].apply(process)
display(df_tourist.head())

df_travel['text2'] = df_travel['text2'].apply(process)
display(df_travel.head())

"""# Data Vectorization

# Bag-of-Words (BoW)
"""

# Inisialisasi CountVectorizer
cv = CountVectorizer(max_features=5000, stop_words='english')

# Fit dan transform teks kursus menjadi vektor
course_vectors = cv.fit_transform(df_tourist['text'])

# Menampilkan fitur (kata unik)
print("Fitur (Kata unik):")
print(cv.get_feature_names_out())

# Menampilkan matriks fitur dalam bentuk array
print("\nMatriks BoW:")
print(course_vectors.toarray())

# Simpan ke CSV
df_bow.to_csv('hasil_bow_tourist.csv', index=False)

# Inisialisasi CountVectorizer
cv = CountVectorizer(max_features=5000, stop_words='english')

# Fit dan transform teks kursus menjadi vektor
course_vectors = cv.fit_transform(df_travel['text2'])

# Menampilkan fitur (kata unik)
print("Fitur (Kata unik):")
print(cv.get_feature_names_out())

# Menampilkan matriks fitur dalam bentuk array
print("\nMatriks BoW:")
print(course_vectors.toarray())
# Simpan ke CSV
df_bow.to_csv('hasil_bow_travel.csv', index=False)

# Fungsi untuk preprocessing
def preprocess(text):
    # Lowercase
    text = text.lower()
    # Tokenize
    tokens = word_tokenize(text)
    # Hapus tanda baca dan angka
    tokens = [word for word in tokens if word.isalpha()]
    # Hapus stopwords
    stop_words = set(stopwords.words('english'))
    tokens = [word for word in tokens if word not in stop_words]
    return ' '.join(tokens)  # Kembalikan string lagi untuk vectorizer

# Preprocess teks
df_tourist['clean_text'] = df_tourist['text'].apply(preprocess)

# Inisialisasi CountVectorizer
vectorizer = CountVectorizer()
X_bow = vectorizer.fit_transform(df_tourist['clean_text'])

# Konversi ke DataFrame agar bisa dilihat
df_bow = pd.DataFrame(X_bow.toarray(), columns=vectorizer.get_feature_names_out())

# Tampilkan sebagian hasil BoW
print(df_bow.head())

# Simpan ke CSV
df_bow.to_csv('hasil_bow.csv', index=False)

# Fungsi untuk preprocessing
def preprocess(text):
    # Lowercase
    text = text.lower()
    # Tokenize
    tokens = word_tokenize(text)
    # Hapus tanda baca dan angka
    tokens = [word for word in tokens if word.isalpha()]
    # Hapus stopwords
    stop_words = set(stopwords.words('english'))
    tokens = [word for word in tokens if word not in stop_words]
    return ' '.join(tokens)  # Kembalikan string lagi untuk vectorizer

# Preprocess teks
df_travel['clean_text2'] = df_travel['text2'].apply(preprocess)

# Inisialisasi CountVectorizer
vectorizer = CountVectorizer()
X_bow = vectorizer.fit_transform(df_travel['clean_text2'])

# Konversi ke DataFrame agar bisa dilihat
df_bow = pd.DataFrame(X_bow.toarray(), columns=vectorizer.get_feature_names_out())

# Tampilkan sebagian hasil BoW
print(df_bow.head())

"""#TF-IDF

Metode ini sama persis dengan metode sebelumnya, hanya saja implementasinya menggunakan Term Frequency-Inverse Document Frequency (TF-IDF). Sama seperti sebelumnya, kita hanya perlu menginisialisasi vectorizer dan implementasikan ke dataset teks. Berikut parameter dasar yang kita gunakan di vectorizer ini,

- ngram_range: Rentang banyak kata yang memberikan konteks tambahan. Karena kita setting banyak kata terendahnya adalah 1 dan tertingginya adalah 2, artinya, kita menggunakan unigram dan bigram (kata tunggal dan pasangan kata).

- max_features: Jumlah maksimum fitur yang disimpan berdasarkan perhitungan TF-IDF.
"""

# Inisialisasi TF-IDF Vectorize
tfidfv = TfidfVectorizer(ngram_range=(1, 2), max_features=5000)

# Fit dan transform teks kursus menjadi vektor
course_vectors = tfidfv.fit_transform(df_tourist['text'])

# Menampilkan fitur (kata unik)
print("Fitur (Kata unik):")
print(tfidfv.get_feature_names_out())

# Menampilkan matriks fitur dalam bentuk array
print("\nMatriks TF-IDF:")
print(course_vectors.toarray())

# Simpan ke CSV
df_tfidf.to_csv('hasil_tfidf_tourist.csv', index=False)

# Inisialisasi TF-IDF Vectorize
tfidfv = TfidfVectorizer(ngram_range=(1, 2), max_features=5000)

# Fit dan transform teks kursus menjadi vektor
course_vectors = tfidfv.fit_transform(df_travel['text2'])

# Menampilkan fitur (kata unik)
print("Fitur (Kata unik):")
print(tfidfv.get_feature_names_out())

# Menampilkan matriks fitur dalam bentuk array
print("\nMatriks TF-IDF:")
print(course_vectors.toarray())

# Simpan ke CSV
df_tfidf.to_csv('hasil_tfidf_travel.csv', index=False)

# Inisialisasi TF-IDF Vectorizer
tfidf_vectorizer = TfidfVectorizer()
X_tfidf = tfidf_vectorizer.fit_transform(df_tourist['clean_text'])

# Konversi hasil ke DataFrame
df_tfidf = pd.DataFrame(X_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())

# Tampilkan hasil TF-IDF pertama
print(df_tfidf.head())

# Inisialisasi TF-IDF Vectorizer
tfidf_vectorizer = TfidfVectorizer()
X_tfidf = tfidf_vectorizer.fit_transform(df_travel['clean_text2'])

# Konversi hasil ke DataFrame
df_tfidf = pd.DataFrame(X_tfidf.toarray(), columns=tfidf_vectorizer.get_feature_names_out())

# Tampilkan hasil TF-IDF pertama
print(df_tfidf.head())

"""# Word2Vec

Metode Word2Vec lumayan berbeda dengan dua metode sebelumnya. Metode ini adalah algoritma yang mengubah kata-kata menjadi vektor berdimensi tetap. Hasil dari penggunaan metode ini adalah word embeddings, yaitu representasi kata dalam ruang vektor. Untuk menggunakannya, teks kata harus berbentuk tokenisasi list of words. Oleh karena itu, kita dapat menggunakan fungsi word_tokenize(), lalu kita ubah tokenisasi tersebut menjadi representasi vektor. Berikut parameter dasar yang kita gunakan dalam Word2Vec,

- sentences: Data teks yang sudah ditokenisasi.
- vector_size: Ukuran vektor untuk setiap kata.
- window: Ukuran konteks kata dari suatu kata melihat dari kiri dan kanannya.
- min_count: Minimum frekuensi kata yang muncul untuk mejadi fitur.
"""

# Preprocessing tokenisasi ke list of words
course_tokens = df_tourist['text'].apply(lambda x: word_tokenize(x)).tolist()

# Inisialisasi model Word2Vec
model_w2v_course = Word2Vec(sentences=course_tokens, vector_size=100, window=3, min_count=1)

# Contoh penggunaan: Melihat vektor dari kata "peru"
print("Vektor kata 'peru':")
print(model_w2v_course.wv["peru"])

# Buat DataFrame semua vektor kata
df_word_vectors = pd.DataFrame(
    {word: model_w2v_course.wv[word] for word in model_w2v_course.wv.index_to_key}
).T  # transpose agar satu kata per baris

# Simpan ke CSV
df_word_vectors.to_csv("word2vec_vectors_tourist.csv")

# Preprocessing tokenisasi ke list of words
course_tokens = df_travel['text2'].apply(lambda x: word_tokenize(x)).tolist()

# Inisialisasi model Word2Vec
model_w2v_course = Word2Vec(sentences=course_tokens, vector_size=100, window=3, min_count=1)

# Contoh penggunaan: Melihat vektor dari kata "peru"
print("Vektor kata 'peru':")
print(model_w2v_course.wv["peru"])

# Simpan semua vektor kata ke CSV
df_word_vectors = pd.DataFrame(
    {word: model_w2v_course.wv[word] for word in model_w2v_course.wv.index_to_key}
).T
df_word_vectors.to_csv("word2vec_vectors_travel.csv")

"""#BERT/Sentence Transformers"""

# Inisialisasi model pretrained BERT
model_bert = SentenceTransformer('paraphrase-MiniLM-L6-v2')

# Encode text kursus menjadi vektor
tourist_vectors = model_bert.encode(df_tourist['text'])

# Menampilkan matriks fitur dalam bentuk array
print("\nMatriks BERT:")
print(tourist_vectors)

# Simpan ke CSV
df_bert_vectors = pd.DataFrame(tourist_vectors)
df_bert_vectors.to_csv("bert_vectors_tourist.csv", index=False)

# Encode text kursus menjadi vektor
travel_vectors = model_bert.encode(df_travel['text2'])

# Menampilkan matriks fitur dalam bentuk array
print("\nMatriks BERT:")
print(travel_vectors)

# Simpan ke CSV
df_bert_vectors = pd.DataFrame(tourist_vectors)
df_bert_vectors.to_csv("bert_vectors_travel.csv", index=False)

"""# 2. Similarity Calculation"""

# Hitung similaritas antara setiap vektor pekerjaan dengan setiap vektor kursus
similarity = cosine_similarity(tourist_vectors, travel_vectors)

# Matrix hasil similaritas vector
print("Matrix Cosine Similarity:")
print(similarity)

"""# 3. Build Recommender System"""

from sklearn.feature_extraction.text import TfidfVectorizer

# Buat TF-IDF vectorizer
tfidf_vectorizer = TfidfVectorizer()

# Fit dan transform ke data kamu
tfidf_matrix = tfidf_vectorizer.fit_transform(df_travel['text2'])

def recommend(text):
    try:
        text_vector = tfidf_vectorizer.transform([text])
        similarity_scores = cosine_similarity(text_vector, tfidf_matrix)[0]

        # Urutkan dari skor tertinggi ke rendah
        idx_sorted = similarity_scores.argsort()[::-1]

        # Ambil hasil unik selain dirinya sendiri (text yg mirip tapi bukan dia)
        top_idx = []
        seen = set()
        for i in idx_sorted:
            item = df_travel['text2'].iloc[i]
            if item != text and item not in seen:
                seen.add(item)
                top_idx.append(i)
            if len(top_idx) == 5:
                break

        return df_travel.iloc[top_idx]['text2'].tolist()
    except Exception:
        idx = np.random.choice(len(df_travel), 5, replace=False)
        return df_travel.iloc[idx]['text2'].drop_duplicates().tolist()

for country in df_travel['text2'].tolist()[:5]:  # atau ganti jumlah sesuai keinginan
    recommended_places = recommend(country)
    print(f"Rekomendasi tempat wisata untuk: {country}")
    for i, place in enumerate(recommended_places, 1):
        print(f"{i}. {place}")
    print()

"""# 4. Evaluation

# 4.1 Manual Inspection
"""

queries = ["afghanistan afg", "aruba abw", "angola ago", "africa western central afw"]

for country_name in queries:
    recommended_places = recommend(country_name)

    print(f"Rekomendasi tempat wisata untuk: {country_name}")
    for i, place in enumerate(recommended_places, start=1):
        print(f"{i}. {place}")
    print()

"""#Ground Truth"""

# Ground truth mapping: key = index df_travel['text'], value = list index df_tourist['text2'] yang relevan
ground_truth = {
    0: [10, 25, 50],
    1: [5, 15, 30],
    2: [12, 28],
}

# Bangun DataFrame ground truth
ground_truth_list = []
for travel_id, tourist_ids in ground_truth.items():
    for tourist_id in tourist_ids:
        ground_truth_list.append({
            'travel_id': travel_id,
            'tourist_id': tourist_id,
            'relevance': 1
        })
df_ground_truth = pd.DataFrame(ground_truth_list)
df_ground_truth

# ----- Simulasi data wisata (df_tourist) -----
import pandas as pd

df_tourist = pd.DataFrame({
    'Place': [
        "Eagle Beach", "Arikok National Park", "California Lighthouse",
        "Oranjestad Market", "Palm Beach", "Baby Beach", "Natural Pool",
        "Alto Vista Chapel", "Butterfly Farm", "Bushiribana Gold Mill Ruins"
    ]
})

# ----- Simulasi data travel (df_travel) -----
df_travel = pd.DataFrame({
    'Country Name': ["aruba abw"]
})

# ----- Fungsi rekomendasi dummy -----
# Anggap kamu sudah punya versi pakai TF-IDF, tapi ini dummy contohnya
def recommend(travel_title):
    if travel_title.lower() == "aruba abw":
        return [
            "Eagle Beach", "Palm Beach", "Natural Pool", "Butterfly Farm", "Oranjestad Market"
        ]
    else:
        return []

# ----- Ground truth relevan -----
ground_truth_places = [
    "Eagle Beach", "California Lighthouse", "Arikok National Park"
]

# ----- Precision@5 Evaluation -----
travel_title = "aruba abw"
top_k = 5

recommended_places = recommend(travel_title)[:top_k]
recommended_indices = df_tourist[df_tourist['Place'].isin(recommended_places)].index.tolist()
ground_truth_indices = df_tourist[df_tourist['Place'].isin(ground_truth_places)].index.tolist()

hits = sum([1 for idx in recommended_indices if idx in ground_truth_indices])
precision_at_5 = hits / top_k

# ----- Output -----
print(f'Precision@5 untuk travel \"{travel_title}\": {precision_at_5:.2f}')
print(f'Index hasil rekomendasi (df_tourist): {recommended_indices}')
print(f'Index tempat relevan dari ground truth: {ground_truth_indices}')